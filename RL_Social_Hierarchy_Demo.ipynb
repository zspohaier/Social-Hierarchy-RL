{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neven-x/Social-Hierarchy-RL/blob/main/RL_Social_Hierarchy_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmu7fw-JNem8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "import functools\n",
        "import random\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from pettingzoo.utils.env import ParallelEnv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq_Xm1J6ysY8",
        "outputId": "216e14d0-540a-48a8-86ee-f7ecd0cd7efc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:542: UserWarning: \u001b[33mWARN: Overriding environment Hierarchy_Grid\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {spec.id}\")\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "class Hierarchy_Grid(ParallelEnv):\n",
        "    metadata = {\n",
        "        \"name\": \"Hierarchy Grid\",\n",
        "    }\n",
        "\n",
        "    def __init__(self, grid_size, num_agents, max_iter):\n",
        "        self.timestep = None\n",
        "        self.grid_size = grid_size\n",
        "        self.agents = np.arange(num_agents)\n",
        "        self.agent_positions = None\n",
        "        self.food_positions = None\n",
        "        self.fight_probs = None\n",
        "        self.rewards = {name: 0 for name in self.agents}\n",
        "\n",
        "        self.observation_space = spaces.MultiDiscrete([self.grid_size, self.grid_size, self.num_agents + 1])\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "\n",
        "        self.food_position_map = np.zeros((self.grid_size, self.grid_size))\n",
        "        self.agent_position_maps = {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.timestep = 0\n",
        "        self.fight_probs = {name: np.zeros(self.num_agents - 1)}\n",
        "\n",
        "        self.agent_positions = {}\n",
        "        self.agent_position_maps = {}\n",
        "        for agent in self.agents:\n",
        "\n",
        "            agent_position = np.random.randint(0, self.grid_size, 2)\n",
        "            self.agent_positions[agent] = agent_position\n",
        "\n",
        "            agent_position_map = np.zeros((self.grid_size, self.grid_size))\n",
        "            agent_position_map[agent_position] = 1\n",
        "            self.agent_position_maps[agent] = agent_position_map\n",
        "\n",
        "        self.food_positions = [np.random.randint(0, self.grid_size, 2)]\n",
        "        self.food_position_map = np.zeros((self.grid_size, self.grid_size))\n",
        "        self.food_position_map[self.food_positions[0]] = 1\n",
        "\n",
        "        observations = np.stack(self.agent_position_maps.values())\n",
        "        observations = np.stack([observations, self.food_position_map])\n",
        "\n",
        "        observations = {name: observations for name in self.agents}\n",
        "        return observations, {}\n",
        "\n",
        "    def step(self, actions):\n",
        "\n",
        "        self.timestep += 1\n",
        "\n",
        "        for agent in self.agents:\n",
        "            action = actions[agent]\n",
        "            self.move_agent(agent, action)\n",
        "\n",
        "        self.two_on_food_tile()\n",
        "        self.feed()\n",
        "        self.calc_reward()\n",
        "\n",
        "        if self.timestep > self.max_iter:\n",
        "            terminations = {name: True for name in self.agents}\n",
        "        else:\n",
        "            terminations = {name: False for name in self.agents}\n",
        "\n",
        "        # Add check if any agents have 0 food in which case they die\n",
        "\n",
        "        observations = np.stack(self.agent_position_maps.values())\n",
        "        observations = np.stack([observations, self.food_position_map])\n",
        "\n",
        "        observations = {name: observations for name in self.agents}\n",
        "\n",
        "        return observations, self.rewards, terminations, _, _\n",
        "\n",
        "\n",
        "    def move_agent(self, agent, action):\n",
        "        # Move the agent based on the selected action\n",
        "        x, y = self.agent_positions[agent]\n",
        "\n",
        "        if action == 0:  # Up\n",
        "            x -= 1\n",
        "        elif action == 1:  # Down\n",
        "            x += 1\n",
        "        elif action == 2:  # Left\n",
        "            y -= 1\n",
        "        elif action == 3:  # Right\n",
        "            y += 1\n",
        "\n",
        "        # Check if the new position is within grid boundaries\n",
        "        if 0 <= x < self.grid_size and 0 <= y < self.grid_size:\n",
        "            self.agent_positions[agent] = (x, y)\n",
        "\n",
        "\n",
        "    def conflict(self, agent1, agent2):\n",
        "\n",
        "        def sig(x):\n",
        "            return 1 / (1 + np.exp(-x))\n",
        "\n",
        "        # Agents make decision to fight or leave\n",
        "        # 1 == fight, 0 == leave\n",
        "        decision1 = bool(np.random.binomial(1, sig(self.stay_probs[agent1][agent2])))\n",
        "        decision2 = bool(np.random.binomial(1, sig(self.stay_probs[agent2][agent1])))\n",
        "\n",
        "\n",
        "        # Outcome of fight is determined in case both decide to stay\n",
        "        outcome = np.random.binomial(1, 0.5)\n",
        "        if outcome == 0:\n",
        "            outcome = -1\n",
        "\n",
        "        if not (decision1 or decision2):\n",
        "            self.relocate_agent(agent1)\n",
        "            self.relocate_agent(agent2)\n",
        "\n",
        "        reward_dict = {(False, False): (0, 0),\n",
        "                       (True, False): (5, 0),\n",
        "                       (False, True): (0, 5),\n",
        "                       (True, True): (5 * outcome, 5 * np.delete([-1,1], outcome))}\n",
        "\n",
        "        # Allocate rewards based on decisions and fight outcome\n",
        "        reward1, reward2 = reward_dict((decision1, decision2))\n",
        "        self.reward[agent1] += reward1\n",
        "        self.reward[agent2] += reward2\n",
        "\n",
        "        # Update future staying probabilities\n",
        "        self.stay_prob[agent1][agent2] += lr * (reward1 - sig(self.stay_prob[agent1][agent2]))\n",
        "        self.stay_prob[agent1][agent2] += lr * (reward2 - sig(self.stay_prob[agent1][agent2]))\n",
        "\n",
        "    def several_on_food_tile(self):\n",
        "\n",
        "        for food_tile in self.food_positions:\n",
        "            agents_on_tile = [agent for agent, position in self.agent_positions.items() if position == food_tile]\n",
        "\n",
        "        if len(agents_on_tile) > 1:\n",
        "\n",
        "            pairs = zip(agents_on_tile[:-1], agents_on_tile[1:])\n",
        "\n",
        "            for pair in pairs:\n",
        "                conflict(pair[0], pair[1])\n",
        "\n",
        "\n",
        "    def relocate_agent(self, agent):\n",
        "        # Relocate the agent to an adjacent position\n",
        "        agent_position = self.agent_positions[agent]\n",
        "        #agent_position_map = self.agent_position_maps[agent]\n",
        "\n",
        "        valid_position = False\n",
        "        step = 1\n",
        "        while not valid_position:\n",
        "\n",
        "            # Generate moves\n",
        "            possible_moves = np.array([[0, 1], [0, -1], [1, 0], [-1, 0]]) * step  # Right, Left, Down, Up\n",
        "\n",
        "            # Check if any of new positions are valid (within gridworld and not already occupied)\n",
        "            for move in possible_moves:\n",
        "                new_position = tuple(map(sum, zip(agent_position, move)))\n",
        "\n",
        "                if 0 <= new_position[0] < self.grid_size and 0 <= new_position[1] < self.grid_size:\n",
        "                    if new_position not in self.agent_positions.values():\n",
        "\n",
        "                        valid_position = True\n",
        "                        self.agent_positions[agent_id] = new_position\n",
        "                        agent_position_map = np.zeros((self.grid_size, self.grid_size))\n",
        "                        agent_position_map[new_position] = 1\n",
        "                        self.agent_positions_maps[agent] = agent_position_map\n",
        "\n",
        "            step += 1\n",
        "\n",
        "    def render(self):\n",
        "        # colormap_food = colors.ListedColormap([\"white\",\"green\"])\n",
        "        # colormap_agent = colors.ListedColormap([\"white\",\"red\"])\n",
        "\n",
        "        fig = plt.figure(figsize=(5,5), frameon=False)\n",
        "\n",
        "        plt.title(\"Grid World\",size=13)\n",
        "        plt.xticks(np.arange(0,self.greed_size,1))\n",
        "        plt.yticks(np.arange(0,self.greed_size,1))\n",
        "\n",
        "        agent_position_map = sum(self.agent_position_maps.values())\n",
        "\n",
        "        plt.imshow(food_position_map, vmax = 2, cmap = 'Greens', alpha=0.4, extent=[0, 10, 0, 10])\n",
        "        plt.imshow(agent_position_map, vmax = 2, cmap = 'Reds', alpha=0.4, extent=[0, 10, 0, 10])\n",
        "\n",
        "        ax = plt.gca();\n",
        "        ax.grid()\n",
        "\n",
        "        plt.show()\n",
        "        plt.figure()\n",
        "\n",
        "gym.register(\n",
        "    id='Hierarchy_Grid',\n",
        "    entry_point=Hierarchy_Grid,\n",
        "    kwargs={'grid_size': 10, 'num_agents': 10, 'max_iter': 200}\n",
        ")\n",
        "\n",
        "env = gym.make('Hierarchy_Grid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI6-NSubtqn5"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_shape, action_size):\n",
        "        def __init__(self, state_shape, action_size):\n",
        "        self.state_shape = state_shape\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 1  # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=self.state_shape))\n",
        "        model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs5aN0qht1i3",
        "outputId": "61e640dc-f44f-46b6-cdb8-3e80840e770f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (10, 10)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('GridWorld')\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "state_shape = (10, 10, 3)\n",
        "\n",
        "dqn = DQNAgent(state_shape, n_actions)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}